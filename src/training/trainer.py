from torch.utils.data import DataLoader
from src.utils.data import TorchDataset
import torch.nn as nn
import torch.optim as optim
import torch
from rich.progress import track
from rich.panel import Panel
import copy
from src.const import LOGGER, DEVICE
import numpy as np
import matplotlib.pyplot as plt
from typing import Optional

# Define plotting style.
plt.style.use("seaborn-v0_8-dark-palette")
plt.rcParams.update(
    {
        "figure.figsize": (12, 6),
        "axes.labelsize": 16,
        "axes.grid": True,
        "xtick.labelsize": 14,
        "ytick.labelsize": 14,
        "axes.titlesize": 18,
        "legend.fontsize": 16,
        "lines.linewidth": 4,
        "text.usetex": False,
        "font.family": "serif",
        "image.cmap": "magma",
    }
)


class Trainer:
    """Class for a trainer for NLP."""

    def __init__(
        self,
        model: nn.Module,
        train_data: TorchDataset,
        eval_data: TorchDataset,
        batch_size: int = 32,
    ) -> None:
        """Initialize the trainer.

        Args:
            model (nn.Module): The model.
            train_data (TorchDataset): The training data.
            eval_data (TorchDataset): The evaluation data.
            batch_size (int, optional): The batch size. Defaults to 32.
        """
        self.model = model.to(DEVICE)
        self.batch_size = batch_size
        self.train_loader = DataLoader(
            train_data,
            batch_size=batch_size,
            shuffle=True,
        )
        self.eval_loader = DataLoader(
            eval_data,
            batch_size=batch_size,
            shuffle=False,
        )

    def reset_history(self):
        """Reset the training history."""
        self.history = {"train_loss": [], "eval_loss": []}

    def train(
        self,
        num_epochs: int = 10,
        learning_rate: float = 1e-3,
        config: dict = {},
        early_stopping: bool = False,
        patience: int = 3,
    ) -> None:
        """Train the model.

        Args:
            num_epochs (int, optional): The number of epochs to train for.
                                        Defaults to 10.
            learning_rate (float, optional): The learning rate to use for
                                             optimization. Defaults to 1e-3.
            config (dict, optional): The configuration for the model. Defaults
                                     to {}.
            early_stopping (bool, optional): Whether to use early stopping for
                                             the training. Defaults to False.
            patience (int, optional): The number of epochs to wait for
                                      improvement, if early stopping is
                                      enabled. Defaults to 3.
        """
        self.reset_history()

        # Set up optimizer and loss function.
        optimizer = config.get("optimizer", optim.Adam)(
            self.model.parameters(), lr=learning_rate
        )
        criterion = config.get("criterion", nn.CrossEntropyLoss)()

        if early_stopping:
            best_eval_loss = float("inf")
            best_model_state = None
            epochs_no_improve = 0

        for epoch in track(range(num_epochs), description=f"Training Epochs"):
            self.model.train()

            total_loss = 0
            for batch in track(self.train_loader, description="Training Batches"):
                # Update model parameters based on the batch.
                inputs, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)

                optimizer.zero_grad()

                outputs = self.model(inputs)
                loss = criterion(outputs, labels)

                loss.backward()
                optimizer.step()
                self.history["train_loss"].append(loss.item())

                total_loss += loss.item()

            avg_loss = total_loss / len(self.train_loader)
            eval_loss = self.evaluate(criterion)

            self.history["eval_loss"].append(eval_loss)

            LOGGER.log_and_print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

            if early_stopping:
                if eval_loss < best_eval_loss:
                    # If the evaluation loss beats the "best" evaluation loss,
                    # save the current best model weights in the memory.
                    best_eval_loss = eval_loss
                    epochs_no_improve = 0
                    best_model_state = copy.deepcopy(self.model.state_dict())

                else:
                    epochs_no_improve += 1

                    LOGGER.info(
                        f"No improvement in validation loss for {epochs_no_improve} epochs."
                    )

                    if epochs_no_improve >= patience:
                        panel = Panel(
                            f"[bold red]Early stopping triggered after {epochs_no_improve+1} epochs without improvement![/bold red]"
                        )

                        LOGGER.log_and_print(panel)
                        break

        if early_stopping and best_model_state is not None:
            # If early stopping occurs, set the best model weights before the
            # end of training.
            self.model.load_state_dict(best_model_state)
            LOGGER.log_and_print("Restored the best model weights from early stopping.")

    def evaluate(self, criterion: nn.Module) -> float:
        """Evaluate the model on the evaluation set.

        Args:
            criterion (nn.Module): The loss function.

        Returns:
            float: The average loss on the evaluation set.
        """
        self.model.eval()

        total_loss = 0

        with torch.no_grad():
            for batch in track(self.eval_loader, description="Evaluating"):
                inputs, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)
                outputs = self.model(inputs)
                loss = criterion(outputs, labels)
                total_loss += loss.item()

        avg_loss = total_loss / len(self.eval_loader)

        return avg_loss

    def plot_history(self, show: bool = True, save_path: Optional[str] = None) -> None:
        """Plot the training and evaluation loss history.

        Args:
            show (bool, optional): Whether to show the plot immediately.
                                   Defaults to True.
            save_path (Optional[str], optional): The path to save the plot to.
                                                 Defaults to None.
        """
        x_train = np.asarray(range(1, len(self.history["train_loss"]) + 1))
        x_eval = np.asarray(range(1, len(self.history["eval_loss"]) + 1))
        x_eval = x_eval * (
            len(self.history["train_loss"]) / len(self.history["eval_loss"])
        )

        plt.figure()
        plt.plot(x_train, self.history["train_loss"], label="Train Loss")
        plt.plot(x_eval, self.history["eval_loss"], label="Eval Loss")
        plt.xlabel("Batches")
        plt.ylabel("Loss")
        plt.title("Training and Evaluation Loss History")
        plt.legend()

        if save_path:
            plt.savefig(save_path)
            LOGGER.info(f"Training history plot saved to {save_path}")

        if show:
            plt.show()

    def save_model(self, path: str) -> None:
        """Save the model.

        Args:
            path (str): The path to save the model to.
        """
        torch.save(self.model.state_dict(), path)
        LOGGER.info(f"Model saved to {path}")

    def load_model(self, path: str) -> None:
        """Load the model.

        Args:
            path (str): The path to load the model from.
        """
        self.model.load_state_dict(torch.load(path))
        self.model.to(DEVICE)
        LOGGER.info(f"Model loaded from {path}")
